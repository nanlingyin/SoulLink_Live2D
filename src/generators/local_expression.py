"""
æœ¬åœ°æ¨¡å‹è¡¨æƒ…ç”Ÿæˆå™¨
ä½¿ç”¨æœ¬åœ° Qwen + LoRA æ¨¡å‹ç”Ÿæˆè¡¨æƒ…å‚æ•°
"""

import re
import json
import time
from pathlib import Path
from typing import Dict, Tuple

from ..config import LLMConfig
from .base import BaseGenerator

# æœ¬åœ°æ¨¡å‹æ”¯æŒï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªå®‰è£…æ—¶æŠ¥é”™ï¼‰
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    LOCAL_MODEL_AVAILABLE = True
except ImportError:
    LOCAL_MODEL_AVAILABLE = False


class LocalExpressionGenerator(BaseGenerator):
    """ä½¿ç”¨æœ¬åœ° Qwen + LoRA æ¨¡å‹ç”Ÿæˆè¡¨æƒ…å‚æ•°"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.available_parameters: Dict[str, dict] = {}
        self.model = None
        self.tokenizer = None
        self._initialized = False

    def _lazy_init(self) -> None:
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶æ‰åŠ è½½æ¨¡å‹ï¼‰"""
        if self._initialized:
            return

        if not LOCAL_MODEL_AVAILABLE:
            raise RuntimeError("æœ¬åœ°æ¨¡å‹ä¾èµ–æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install torch transformers peft")

        base_path = Path(self.config.local_base_model_path).resolve()
        lora_path = Path(self.config.local_lora_model_path).resolve()

        print(f"ğŸ”„ åŠ è½½æœ¬åœ°æ¨¡å‹...")
        print(f"   åŸºç¡€æ¨¡å‹: {base_path}")
        print(f"   LoRA æƒé‡: {lora_path}")

        if not base_path.exists():
            raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹ä¸å­˜åœ¨: {base_path}")
        if not lora_path.exists():
            raise FileNotFoundError(f"LoRA æ¨¡å‹ä¸å­˜åœ¨: {lora_path}")

        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(base_path),
            trust_remote_code=True
        )

        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            str(base_path),
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=self.config.local_device
        )

        # åŠ è½½ LoRA æƒé‡
        self.model = PeftModel.from_pretrained(
            base_model,
            str(lora_path),
            torch_dtype=torch.float16
        )

        self.model.eval()
        self._initialized = True
        print(f"âœ… æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆ")

    def update_parameters(self, parameters: Dict[str, dict]) -> None:
        """æ›´æ–°å¯ç”¨å‚æ•°åˆ—è¡¨"""
        self.available_parameters = parameters
        print(f"ğŸ­ å‚æ•°å·²æ›´æ–°: {len(parameters)} ä¸ªå‚æ•°")

    def _build_prompt(self, emotion: str, intensity: float = 0.8) -> str:
        """æ„å»ºæ¨ç†æç¤º"""
        param_desc = ", ".join([
            f"{pid}[{info.get('min', -30)},{info.get('max', 30)}]"
            for pid, info in list(self.available_parameters.items())[:8]
        ])
        if len(self.available_parameters) > 8:
            param_desc += f" ...å…±{len(self.available_parameters)}ä¸ªå‚æ•°"

        system_prompt = f"""ä½ æ˜¯ Live2D å‚æ•°æ˜ å°„ä¸“å®¶ã€‚æ ¹æ®æƒ…æ„Ÿç”Ÿæˆå‚æ•°JSONã€‚
å¯ç”¨å‚æ•°: {param_desc}
åªè¿”å›JSONæ ¼å¼ã€‚"""

        user_input = json.dumps({
            "emotion": emotion,
            "intensity": intensity,
            "params": list(self.available_parameters.keys())
        }, ensure_ascii=False)

        return f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""

    def _extract_emotion(self, text: str) -> Tuple[str, float]:
        """ä»æ–‡æœ¬ä¸­æå–æƒ…æ„Ÿå’Œå¼ºåº¦"""
        # ç®€å•çš„æƒ…æ„Ÿæ˜ å°„
        emotion_keywords = {
            "å¼€å¿ƒ": ("happy", 0.8),
            "é«˜å…´": ("happy", 0.7),
            "å¿«ä¹": ("happy", 0.8),
            "å“ˆå“ˆ": ("happy", 0.9),
            "æ‚²ä¼¤": ("sad", 0.7),
            "éš¾è¿‡": ("sad", 0.6),
            "ä¼¤å¿ƒ": ("sad", 0.8),
            "å“­": ("sad", 0.9),
            "ç”Ÿæ°”": ("angry", 0.8),
            "æ„¤æ€’": ("angry", 0.9),
            "çƒ¦": ("annoyed", 0.6),
            "æƒŠè®¶": ("surprised", 0.8),
            "åƒæƒŠ": ("surprised", 0.7),
            "å®³ç¾": ("shy", 0.7),
            "ä¸å¥½æ„æ€": ("shy", 0.5),
            "è„¸çº¢": ("shy", 0.8),
            "æ€è€ƒ": ("thinking", 0.6),
            "å—¯": ("thinking", 0.5),
            "å›°": ("sleepy", 0.7),
            "ç´¯": ("sleepy", 0.6),
            "å…´å¥‹": ("excited", 0.8),
            "æ‹…å¿ƒ": ("worried", 0.6),
            "ç´§å¼ ": ("worried", 0.7),
            "å›°æƒ‘": ("confused", 0.6),
            "ç–‘æƒ‘": ("confused", 0.5),
        }

        text_lower = text.lower()
        for keyword, (emotion, intensity) in emotion_keywords.items():
            if keyword in text_lower:
                return emotion, intensity

        # é»˜è®¤è¿”å›ä¸­æ€§/å¹³é™
        return "neutral", 0.5

    async def generate(self, input_text: str, context: str = "") -> dict:
        """ç”Ÿæˆè¡¨æƒ…å‚æ•°"""
        self._lazy_init()

        if not self.available_parameters:
            raise ValueError("æ¨¡å‹å‚æ•°å°šæœªåŠ è½½")

        # ä»è¾“å…¥æ–‡æœ¬æå–æƒ…æ„Ÿ
        emotion, intensity = self._extract_emotion(input_text)
        print(f"ğŸ­ [æœ¬åœ°æ¨¡å‹] æ£€æµ‹æƒ…æ„Ÿ: {emotion} (å¼ºåº¦: {intensity})")

        # æ„å»ºæç¤º
        prompt = self._build_prompt(emotion, intensity)

        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)

        # ç”Ÿæˆ
        start_time = time.time()

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.local_max_new_tokens,
                temperature=self.config.local_temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        inference_time = (time.time() - start_time) * 1000
        print(f"ğŸ­ [æœ¬åœ°æ¨¡å‹] æ¨ç†å®Œæˆ â±ï¸ {inference_time:.0f}ms")

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # è§£æ JSON
        try:
            result = json.loads(generated_text)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON
            json_match = re.search(r'\{[\s\S]*\}', generated_text)
            if json_match:
                try:
                    result = json.loads(json_match.group(0))
                except:
                    result = {"expression": emotion, "parameters": {}, "duration": 600}
            else:
                result = {"expression": emotion, "parameters": {}, "duration": 600}

        # éªŒè¯å‚æ•°èŒƒå›´
        validated_params = {}
        for param_id, value in result.get('parameters', {}).items():
            if param_id in self.available_parameters:
                param_info = self.available_parameters[param_id]
                try:
                    num_value = float(value)
                    num_value = max(param_info.get('min', -30), min(param_info.get('max', 30), num_value))
                    validated_params[param_id] = round(num_value, 3)
                except (ValueError, TypeError):
                    continue

        result['parameters'] = validated_params
        result['expression'] = result.get('expression', emotion)
        result['duration'] = result.get('duration', 600)

        return result

    def is_available(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if not LOCAL_MODEL_AVAILABLE:
            return False
        base_path = Path(self.config.local_base_model_path).resolve()
        lora_path = Path(self.config.local_lora_model_path).resolve()
        return base_path.exists() and lora_path.exists()
