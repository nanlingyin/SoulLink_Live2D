"""
SoulLink_Live2D - AI é©±åŠ¨çš„ Live2D è¡¨æƒ…æ§åˆ¶ç³»ç»Ÿ
Python åç«¯æœåŠ¡å™¨

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ‰«æ l2d ç›®å½•ï¼Œå‘ç°æ‰€æœ‰ Live2D æ¨¡å‹
2. ç›‘å¬æ–‡ä»¶å¤¹å˜åŒ–ï¼Œå®æ—¶é€šçŸ¥å‰ç«¯
3. WebSocket æœåŠ¡ï¼Œä¸å‰ç«¯åŒå‘é€šä¿¡
4. LLM è°ƒç”¨ï¼Œç”Ÿæˆè¡¨æƒ…å‚æ•°ï¼ˆæ”¯æŒæœ¬åœ°æ¨¡å‹å’Œè¿œç¨‹APIï¼‰
5. ç»Ÿä¸€é…ç½®ç®¡ç† - æ‰€æœ‰é…ç½®ä» config.yaml è¯»å–
"""

import asyncio
import json
import os
import re
import time
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass, asdict, field
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import aiohttp
from aiohttp import web
import aiohttp_cors
import yaml

# æœ¬åœ°æ¨¡å‹æ”¯æŒï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªå®‰è£…æ—¶æŠ¥é”™ï¼‰
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    LOCAL_MODEL_AVAILABLE = True
except ImportError:
    LOCAL_MODEL_AVAILABLE = False
    print("âš ï¸ æœ¬åœ°æ¨¡å‹ä¾èµ–æœªå®‰è£… (torch/transformers/peft)ï¼Œå°†ä½¿ç”¨ API æ¨¡å¼")

# ============================================
# é…ç½®ç®¡ç† - æ‰€æœ‰é…ç½®ä» config.yaml ç»Ÿä¸€è¯»å–
# ============================================

@dataclass
class LLMConfig:
    """LLM API é…ç½®"""
    mode: str = "api"  # "local" æˆ– "api"
    provider: str = "openai"
    api_key: str = ""
    base_url: str = "https://api.openai.com/v1"
    model: str = "gpt-4o-mini"
    temperature: float = 0.7
    max_tokens: int = 500
    # æœ¬åœ°æ¨¡å‹é…ç½®
    local_base_model_path: str = ""
    local_lora_model_path: str = ""
    local_device: str = "auto"
    local_temperature: float = 0.1
    local_max_new_tokens: int = 512

@dataclass  
class ServerConfig:
    """æœåŠ¡å™¨é…ç½®"""
    host: str = "0.0.0.0"
    port: int = 3000
    model_dirs: List[str] = field(default_factory=lambda: ["./l2d"])

@dataclass
class AnimationConfig:
    """åŠ¨ç”»é…ç½®"""
    default_duration: int = 1000
    easing: str = "easeInOutCubic"
    auto_reset_delay: int = 1500

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    directory: str = "./l2d"
    default_scale: float = 0.8

@dataclass
class UIConfig:
    """ç•Œé¢é…ç½®"""
    show_control_panel: bool = True
    show_physics_params: bool = False
    default_background: int = 0

class ConfigManager:
    """
    ç»Ÿä¸€é…ç½®ç®¡ç†å™¨
    æ‰€æœ‰é…ç½®ä» config.yaml è¯»å–ï¼Œç”¨æˆ·æ— éœ€ä¿®æ”¹ä»£ç 
    """
    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = config_path
        self.llm = LLMConfig()
        self.server = ServerConfig()
        self.animation = AnimationConfig()
        self.model = ModelConfig()
        self.ui = UIConfig()
        self._raw_config = {}  # ä¿å­˜åŸå§‹é…ç½®ç”¨äºå‰ç«¯
        self.load()
    
    def load(self):
        """ä» config.yaml åŠ è½½æ‰€æœ‰é…ç½®"""
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    self._raw_config = yaml.safe_load(f) or {}
                
                # åŠ è½½ LLM é…ç½®
                llm_data = self._raw_config.get('llm', {})
                api_data = llm_data.get('api', {})
                local_data = llm_data.get('local', {})

                self.llm = LLMConfig(
                    mode=llm_data.get('mode', 'api'),
                    provider=api_data.get('provider', 'openai'),
                    api_key=api_data.get('apiKey', ''),
                    base_url=api_data.get('baseUrl', 'https://api.openai.com/v1'),
                    model=api_data.get('model', 'gpt-4o-mini'),
                    temperature=api_data.get('temperature', 0.7),
                    max_tokens=api_data.get('maxTokens', 500),
                    # æœ¬åœ°æ¨¡å‹é…ç½®
                    local_base_model_path=local_data.get('baseModelPath', './ct2model/models/qwen2.5-1.5b-instruct'),
                    local_lora_model_path=local_data.get('loraModelPath', './ct2model/output/l2d-motion-lora/final'),
                    local_device=local_data.get('device', 'auto'),
                    local_temperature=local_data.get('temperature', 0.1),
                    local_max_new_tokens=local_data.get('maxNewTokens', 512)
                )
                
                # åŠ è½½æœåŠ¡å™¨é…ç½®
                server_data = self._raw_config.get('server', {})
                model_dir = self._raw_config.get('model', {}).get('directory', './l2d')
                self.server = ServerConfig(
                    host=server_data.get('host', '0.0.0.0'),
                    port=server_data.get('port', 3000),
                    model_dirs=server_data.get('modelDirs', [model_dir])
                )
                
                # åŠ è½½åŠ¨ç”»é…ç½®
                anim_data = self._raw_config.get('animation', {})
                self.animation = AnimationConfig(
                    default_duration=anim_data.get('defaultDuration', 1000),
                    easing=anim_data.get('easing', 'easeInOutCubic'),
                    auto_reset_delay=anim_data.get('autoResetDelay', 1500)
                )
                
                # åŠ è½½æ¨¡å‹é…ç½®
                model_data = self._raw_config.get('model', {})
                self.model = ModelConfig(
                    directory=model_data.get('directory', './l2d'),
                    default_scale=model_data.get('defaultScale', 0.8)
                )
                
                # åŠ è½½ UI é…ç½®
                ui_data = self._raw_config.get('ui', {})
                self.ui = UIConfig(
                    show_control_panel=ui_data.get('showControlPanel', True),
                    show_physics_params=ui_data.get('showPhysicsParams', False),
                    default_background=ui_data.get('defaultBackground', 0)
                )
                
                print(f"âœ… é…ç½®å·²åŠ è½½: {self.config_path}")
                print(f"   ğŸ¤– LLM æ¨¡å¼: {self.llm.mode}")
                if self.llm.mode == "local":
                    print(f"   ğŸ“¦ æœ¬åœ°æ¨¡å‹: {self.llm.local_base_model_path}")
                    print(f"   ğŸ”§ LoRA: {self.llm.local_lora_model_path}")
                else:
                    print(f"   ğŸŒ API: {self.llm.model} @ {self.llm.base_url}")
                print(f"   ğŸ¬ åŠ¨ç”»: duration={self.animation.default_duration}ms, easing={self.animation.easing}")
                
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        else:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    def get_frontend_config(self) -> dict:
        """
        è·å–å‰ç«¯éœ€è¦çš„é…ç½®ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯å¦‚ API Keyï¼‰
        å‰ç«¯é€šè¿‡ /api/config è·å–è¿™äº›é…ç½®
        """
        return {
            "server": {
                "host": self.server.host,
                "port": self.server.port,
                "modelDirs": self.server.model_dirs
            },
            "llm": {
                "mode": self.llm.mode,
                "provider": self.llm.provider if self.llm.mode == "api" else "local",
                "model": self.llm.model if self.llm.mode == "api" else "qwen2.5-1.5b-lora",
                # ä¸æš´éœ² api_key å’Œ base_url
            },
            "animation": {
                "defaultDuration": self.animation.default_duration,
                "easing": self.animation.easing,
                "autoResetDelay": self.animation.auto_reset_delay
            },
            "model": {
                "directory": self.model.directory,
                "defaultScale": self.model.default_scale
            },
            "ui": {
                "showControlPanel": self.ui.show_control_panel,
                "showPhysicsParams": self.ui.show_physics_params,
                "defaultBackground": self.ui.default_background
            }
        }

# ============================================
# æ¨¡å‹æ‰«æå™¨
# ============================================

@dataclass
class Live2DModel:
    name: str
    path: str
    directory: str
    model_file: str
    cdi_file: Optional[str] = None
    physics_file: Optional[str] = None
    pose_file: Optional[str] = None
    motions: List[str] = None
    
    def __post_init__(self):
        if self.motions is None:
            self.motions = []

class ModelScanner:
    def __init__(self, base_dirs: List[str]):
        self.base_dirs = [Path(d).resolve() for d in base_dirs]
        self.models: Dict[str, Live2DModel] = {}
    
    def scan_all(self) -> Dict[str, Live2DModel]:
        """æ‰«ææ‰€æœ‰ç›®å½•ï¼Œå‘ç° Live2D æ¨¡å‹"""
        self.models.clear()
        
        for base_dir in self.base_dirs:
            if not base_dir.exists():
                print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {base_dir}")
                continue
            
            self._scan_directory(base_dir, base_dir)
        
        print(f"ğŸ” æ‰«æå®Œæˆï¼Œå‘ç° {len(self.models)} ä¸ªæ¨¡å‹")
        for name, model in self.models.items():
            print(f"   - {name}: {model.path}")
        
        return self.models
    
    def _scan_directory(self, directory: Path, base_dir: Path):
        """é€’å½’æ‰«æç›®å½•"""
        try:
            for item in directory.iterdir():
                if item.is_file() and item.suffix == '.json' and item.name.endswith('.model3.json'):
                    model = self._parse_model(item, base_dir)
                    if model:
                        self.models[model.name] = model
                elif item.is_dir() and not item.name.startswith('.'):
                    self._scan_directory(item, base_dir)
        except PermissionError:
            pass
    
    def _parse_model(self, model_file: Path, base_dir: Path) -> Optional[Live2DModel]:
        """è§£ææ¨¡å‹æ–‡ä»¶"""
        try:
            model_dir = model_file.parent
            model_name = model_file.name.replace('.model3.json', '')
            
            # æŸ¥æ‰¾ç›¸å…³æ–‡ä»¶
            cdi_file = None
            physics_file = None
            pose_file = None
            motions = []
            
            for f in model_dir.iterdir():
                if f.suffix == '.json':
                    if f.name.endswith('.cdi3.json'):
                        cdi_file = f.name
                    elif f.name.endswith('.physics3.json'):
                        physics_file = f.name
                    elif f.name.endswith('.pose3.json'):
                        pose_file = f.name
            
            # æŸ¥æ‰¾åŠ¨ä½œæ–‡ä»¶
            motions_dir = model_dir / 'motions'
            if motions_dir.exists():
                motions = [f.name for f in motions_dir.iterdir() 
                          if f.suffix == '.json' and f.name.endswith('.motion3.json')]
            
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ - ä½¿ç”¨ base_dir çš„ç›®å½•åä½œä¸ºå‰ç¼€
            # ä¾‹å¦‚: base_dir = F:\ai_controlled_l2d\l2d
            # model_file = F:\ai_controlled_l2d\l2d\amane.model3.json
            # ç»“æœ: l2d/amane.model3.json
            dir_name = base_dir.name  # "l2d"
            relative_model_path = model_file.relative_to(base_dir)
            relative_dir_path = model_dir.relative_to(base_dir)
            
            # æ„å»ºå®Œæ•´çš„ç›¸å¯¹è·¯å¾„ï¼ˆåŒ…å«ç›®å½•åï¼‰
            full_path = f"{dir_name}/{relative_model_path}".replace('\\', '/')
            full_dir = f"{dir_name}/{relative_dir_path}".replace('\\', '/') if str(relative_dir_path) != '.' else dir_name
            
            return Live2DModel(
                name=model_name,
                path=full_path,
                directory=full_dir,
                model_file=model_file.name,
                cdi_file=cdi_file,
                physics_file=physics_file,
                pose_file=pose_file,
                motions=motions
            )
        except Exception as e:
            print(f"âš ï¸ è§£ææ¨¡å‹å¤±è´¥ {model_file}: {e}")
            return None

# ============================================
# æ–‡ä»¶ç›‘å¬å™¨
# ============================================

class ModelWatcher(FileSystemEventHandler):
    def __init__(self, scanner: ModelScanner, on_change_callback):
        self.scanner = scanner
        self.on_change = on_change_callback
        self._debounce_task = None
    
    def on_any_event(self, event):
        if event.is_directory:
            return
        
        # åªå…³æ³¨ model3.json æ–‡ä»¶çš„å˜åŒ–
        if event.src_path.endswith('.model3.json'):
            # é˜²æŠ–ï¼šé¿å…çŸ­æ—¶é—´å†…å¤šæ¬¡è§¦å‘
            if self._debounce_task:
                self._debounce_task.cancel()
            
            loop = asyncio.get_event_loop()
            self._debounce_task = loop.call_later(1.0, self._handle_change)
    
    def _handle_change(self):
        print("ğŸ“ æ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶å˜åŒ–ï¼Œé‡æ–°æ‰«æ...")
        self.scanner.scan_all()
        if self.on_change:
            asyncio.create_task(self.on_change())

# ============================================
# æœ¬åœ°æ¨¡å‹è¡¨æƒ…ç”Ÿæˆå™¨
# ============================================

class LocalExpressionGenerator:
    """ä½¿ç”¨æœ¬åœ° Qwen + LoRA æ¨¡å‹ç”Ÿæˆè¡¨æƒ…å‚æ•°"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.available_parameters: Dict[str, dict] = {}
        self.model = None
        self.tokenizer = None
        self._initialized = False

    def _lazy_init(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶æ‰åŠ è½½æ¨¡å‹ï¼‰"""
        if self._initialized:
            return

        if not LOCAL_MODEL_AVAILABLE:
            raise RuntimeError("æœ¬åœ°æ¨¡å‹ä¾èµ–æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install torch transformers peft")

        base_path = Path(self.config.local_base_model_path).resolve()
        lora_path = Path(self.config.local_lora_model_path).resolve()

        print(f"ğŸ”„ åŠ è½½æœ¬åœ°æ¨¡å‹...")
        print(f"   åŸºç¡€æ¨¡å‹: {base_path}")
        print(f"   LoRA æƒé‡: {lora_path}")

        if not base_path.exists():
            raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹ä¸å­˜åœ¨: {base_path}")
        if not lora_path.exists():
            raise FileNotFoundError(f"LoRA æ¨¡å‹ä¸å­˜åœ¨: {lora_path}")

        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(base_path),
            trust_remote_code=True
        )

        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            str(base_path),
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=self.config.local_device
        )

        # åŠ è½½ LoRA æƒé‡
        self.model = PeftModel.from_pretrained(
            base_model,
            str(lora_path),
            torch_dtype=torch.float16
        )

        self.model.eval()
        self._initialized = True
        print(f"âœ… æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆ")

    def update_parameters(self, parameters: Dict[str, dict]):
        """æ›´æ–°å¯ç”¨å‚æ•°åˆ—è¡¨"""
        self.available_parameters = parameters
        print(f"ğŸ­ å‚æ•°å·²æ›´æ–°: {len(parameters)} ä¸ªå‚æ•°")

    def _build_prompt(self, emotion: str, intensity: float = 0.8) -> str:
        """æ„å»ºæ¨ç†æç¤º"""
        param_desc = ", ".join([
            f"{pid}[{info.get('min', -30)},{info.get('max', 30)}]"
            for pid, info in list(self.available_parameters.items())[:8]
        ])
        if len(self.available_parameters) > 8:
            param_desc += f" ...å…±{len(self.available_parameters)}ä¸ªå‚æ•°"

        system_prompt = f"""ä½ æ˜¯ Live2D å‚æ•°æ˜ å°„ä¸“å®¶ã€‚æ ¹æ®æƒ…æ„Ÿç”Ÿæˆå‚æ•°JSONã€‚
å¯ç”¨å‚æ•°: {param_desc}
åªè¿”å›JSONæ ¼å¼ã€‚"""

        user_input = json.dumps({
            "emotion": emotion,
            "intensity": intensity,
            "params": list(self.available_parameters.keys())
        }, ensure_ascii=False)

        return f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""

    def _extract_emotion(self, text: str) -> tuple:
        """ä»æ–‡æœ¬ä¸­æå–æƒ…æ„Ÿå’Œå¼ºåº¦"""
        # ç®€å•çš„æƒ…æ„Ÿæ˜ å°„
        emotion_keywords = {
            "å¼€å¿ƒ": ("happy", 0.8),
            "é«˜å…´": ("happy", 0.7),
            "å¿«ä¹": ("happy", 0.8),
            "å“ˆå“ˆ": ("happy", 0.9),
            "æ‚²ä¼¤": ("sad", 0.7),
            "éš¾è¿‡": ("sad", 0.6),
            "ä¼¤å¿ƒ": ("sad", 0.8),
            "å“­": ("sad", 0.9),
            "ç”Ÿæ°”": ("angry", 0.8),
            "æ„¤æ€’": ("angry", 0.9),
            "çƒ¦": ("annoyed", 0.6),
            "æƒŠè®¶": ("surprised", 0.8),
            "åƒæƒŠ": ("surprised", 0.7),
            "å®³ç¾": ("shy", 0.7),
            "ä¸å¥½æ„æ€": ("shy", 0.5),
            "è„¸çº¢": ("shy", 0.8),
            "æ€è€ƒ": ("thinking", 0.6),
            "å—¯": ("thinking", 0.5),
            "å›°": ("sleepy", 0.7),
            "ç´¯": ("sleepy", 0.6),
            "å…´å¥‹": ("excited", 0.8),
            "æ‹…å¿ƒ": ("worried", 0.6),
            "ç´§å¼ ": ("worried", 0.7),
            "å›°æƒ‘": ("confused", 0.6),
            "ç–‘æƒ‘": ("confused", 0.5),
        }

        text_lower = text.lower()
        for keyword, (emotion, intensity) in emotion_keywords.items():
            if keyword in text_lower:
                return emotion, intensity

        # é»˜è®¤è¿”å›ä¸­æ€§/å¹³é™
        return "neutral", 0.5

    async def generate(self, input_text: str, context: str = "") -> dict:
        """ç”Ÿæˆè¡¨æƒ…å‚æ•°"""
        self._lazy_init()

        if not self.available_parameters:
            raise ValueError("æ¨¡å‹å‚æ•°å°šæœªåŠ è½½")

        # ä»è¾“å…¥æ–‡æœ¬æå–æƒ…æ„Ÿ
        emotion, intensity = self._extract_emotion(input_text)
        print(f"ğŸ­ [æœ¬åœ°æ¨¡å‹] æ£€æµ‹æƒ…æ„Ÿ: {emotion} (å¼ºåº¦: {intensity})")

        # æ„å»ºæç¤º
        prompt = self._build_prompt(emotion, intensity)

        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)

        # ç”Ÿæˆ
        start_time = time.time()

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.local_max_new_tokens,
                temperature=self.config.local_temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        inference_time = (time.time() - start_time) * 1000
        print(f"ğŸ­ [æœ¬åœ°æ¨¡å‹] æ¨ç†å®Œæˆ â±ï¸ {inference_time:.0f}ms")

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # è§£æ JSON
        try:
            result = json.loads(generated_text)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON
            json_match = re.search(r'\{[\s\S]*\}', generated_text)
            if json_match:
                try:
                    result = json.loads(json_match.group(0))
                except:
                    result = {"expression": emotion, "parameters": {}, "duration": 600}
            else:
                result = {"expression": emotion, "parameters": {}, "duration": 600}

        # éªŒè¯å‚æ•°èŒƒå›´
        validated_params = {}
        for param_id, value in result.get('parameters', {}).items():
            if param_id in self.available_parameters:
                param_info = self.available_parameters[param_id]
                try:
                    num_value = float(value)
                    num_value = max(param_info.get('min', -30), min(param_info.get('max', 30), num_value))
                    validated_params[param_id] = round(num_value, 3)
                except (ValueError, TypeError):
                    continue

        result['parameters'] = validated_params
        result['expression'] = result.get('expression', emotion)
        result['duration'] = result.get('duration', 600)

        return result

    def is_available(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if not LOCAL_MODEL_AVAILABLE:
            return False
        base_path = Path(self.config.local_base_model_path).resolve()
        lora_path = Path(self.config.local_lora_model_path).resolve()
        return base_path.exists() and lora_path.exists()


# ============================================
# LLM è¡¨æƒ…ç”Ÿæˆå™¨ (API æ¨¡å¼)
# ============================================

class ExpressionGenerator:
    def __init__(self, config: LLMConfig):
        self.config = config
        self.available_parameters: Dict[str, dict] = {}
    
    def update_parameters(self, parameters: Dict[str, dict]):
        """æ›´æ–°å¯ç”¨å‚æ•°åˆ—è¡¨"""
        self.available_parameters = parameters
        print(f"ğŸ­ å‚æ•°å·²æ›´æ–°: {len(parameters)} ä¸ªå‚æ•°")
    
    def _generate_system_prompt(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿæç¤ºè¯"""
        if not self.available_parameters:
            return "æ¨¡å‹å‚æ•°å°šæœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        param_descriptions = "\n".join([
            f"  - {pid}: {info.get('name', pid)}, èŒƒå›´[{info.get('min', -30)}, {info.get('max', 30)}]"
            for pid, info in self.available_parameters.items()
        ])
        
        return f"""ä½ æ˜¯ä¸€ä¸ª Live2D è™šæ‹Ÿå½¢è±¡çš„è¡¨æƒ…æ§åˆ¶å™¨ã€‚æ ¹æ®åœºæ™¯ã€å¯¹è¯æˆ–æƒ…æ„Ÿæè¿°ï¼Œç”Ÿæˆè¡¨æƒ…å‚æ•°ã€‚

å½“å‰æ¨¡å‹å¯ç”¨å‚æ•°ï¼š
{param_descriptions}

è¿”å›JSONæ ¼å¼ï¼š
{{
  "expression": "è¡¨æƒ…æè¿°",
  "parameters": {{
    "å‚æ•°ID": æ•°å€¼,
    ...
  }},
  "duration": è¿‡æ¸¡æ—¶é—´æ¯«ç§’æ•°
}}

è¦æ±‚ï¼š
1. å‚æ•°å€¼è¦è¶³å¤Ÿå¤§ï¼Œè®©è¡¨æƒ…å˜åŒ–æ˜æ˜¾å¯è§
2. å……åˆ†ç»„åˆå¤šä¸ªå‚æ•°æ¥è¡¨è¾¾ä¸°å¯Œè¡¨æƒ…
3. çœ¼ç›ã€çœ‰æ¯›ã€å˜´å·´çš„é…åˆå¯¹è¡¨æƒ…å¾ˆé‡è¦
4. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—"""
    
    async def generate(self, input_text: str, context: str = "") -> dict:
        """è°ƒç”¨ LLM ç”Ÿæˆè¡¨æƒ…å‚æ•°"""
        if not self.config.api_key or self.config.api_key == "your-api-key-here":
            raise ValueError("è¯·å…ˆåœ¨ config.yaml ä¸­è®¾ç½® API Key")

        if not self.available_parameters:
            raise ValueError("æ¨¡å‹å‚æ•°å°šæœªåŠ è½½")

        user_message = f"åœºæ™¯èƒŒæ™¯ï¼š{context}\n\nå½“å‰è¾“å…¥ï¼š{input_text}" if context else input_text

        request_body = {
            "model": self.config.model,
            "messages": [
                {"role": "system", "content": self._generate_system_prompt()},
                {"role": "user", "content": user_message}
            ],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }

        print(f"ğŸ­ [è¡¨æƒ…ç”Ÿæˆ] è°ƒç”¨ API ({self.config.model})...")
        start_time = time.time()

        async with aiohttp.ClientSession() as session:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.config.api_key}"
            }

            async with session.post(
                f"{self.config.base_url}/chat/completions",
                headers=headers,
                json=request_body
            ) as response:
                if response.status != 200:
                    error = await response.text()
                    raise Exception(f"API è¯·æ±‚å¤±è´¥: {response.status} - {error}")

                data = await response.json()
                content = data["choices"][0]["message"]["content"]

                elapsed_time = (time.time() - start_time) * 1000
                print(f"ğŸ­ [è¡¨æƒ…ç”Ÿæˆ] å®Œæˆ â±ï¸ {elapsed_time:.0f}ms")

                # æå– JSON
                json_match = re.search(r'\{[\s\S]*\}', content)
                if not json_match:
                    raise ValueError("æ— æ³•è§£æ LLM è¿”å›çš„ JSON")

                result = json.loads(json_match.group())

                # éªŒè¯å‚æ•°èŒƒå›´
                validated_params = {}
                for param_id, value in result.get("parameters", {}).items():
                    if param_id in self.available_parameters:
                        info = self.available_parameters
                        validated_params[param_id] = max(
                            info.get("min", -30),
                            min(info.get("max", 30), value)
                        )

                result["parameters"] = validated_params
                return result


# ============================================
# LLM èŠå¤©ç”Ÿæˆå™¨
# ============================================

class ChatGenerator:
    """èŠå¤©å¯¹è¯ç”Ÿæˆå™¨"""
    def __init__(self, config: LLMConfig):
        self.config = config
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªå¯çˆ±ã€æ´»æ³¼çš„è™šæ‹ŸåŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´ã€å‹å¥½çš„æ–¹å¼å›å¤ç”¨æˆ·ã€‚
å›å¤è¦æ±‚ï¼š
1. è¯­è¨€è‡ªç„¶ã€æœ‰ä¸ªæ€§
2. é€‚å½“ä½¿ç”¨è¯­æ°”è¯å’Œè¡¨æƒ…
3. å›å¤ä¸è¦å¤ªé•¿ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…
4. æ ¹æ®å¯¹è¯æƒ…ç»ªç»™å‡ºç›¸åº”çš„å›å¤é£æ ¼"""
    
    async def generate(self, message: str, history: list = None) -> str:
        """ç”ŸæˆèŠå¤©å›å¤"""
        if not self.config.api_key or self.config.api_key == "your-api-key-here":
            raise ValueError("è¯·å…ˆåœ¨ config.yaml ä¸­è®¾ç½® API Key")

        messages = [{"role": "system", "content": self.system_prompt}]

        # æ·»åŠ å†å²å¯¹è¯
        if history:
            for h in history[-6:]:  # åªä¿ç•™æœ€è¿‘6æ¡
                messages.append({
                    "role": h.get("role", "user"),
                    "content": h.get("content", "")
                })

        # æ·»åŠ å½“å‰æ¶ˆæ¯ï¼ˆå¦‚æœä¸åœ¨å†å²ä¸­ï¼‰
        if not history or history[-1].get("content") != message:
            messages.append({"role": "user", "content": message})

        request_body = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": 200
        }

        print(f"ğŸ’¬ [èŠå¤©å›å¤] è°ƒç”¨ API ({self.config.model})...")
        start_time = time.time()

        async with aiohttp.ClientSession() as session:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.config.api_key}"
            }

            async with session.post(
                f"{self.config.base_url}/chat/completions",
                headers=headers,
                json=request_body
            ) as response:
                if response.status != 200:
                    error = await response.text()
                    raise Exception(f"èŠå¤© API è¯·æ±‚å¤±è´¥: {response.status} - {error}")

                data = await response.json()
                reply = data["choices"][0]["message"]["content"]

                elapsed_time = (time.time() - start_time) * 1000
                print(f"ğŸ’¬ [èŠå¤©å›å¤] å®Œæˆ â±ï¸ {elapsed_time:.0f}ms | å›å¤: {reply[:30]}...")

                return reply

# ============================================
# WebSocket æœåŠ¡å™¨
# ============================================

class SoulLinkServer:
    def __init__(self, config: ConfigManager):
        self.config = config
        self.scanner = ModelScanner(config.server.model_dirs)

        # æ ¹æ®é…ç½®é€‰æ‹©è¡¨æƒ…ç”Ÿæˆå™¨
        if config.llm.mode == "local":
            local_gen = LocalExpressionGenerator(config.llm)
            if local_gen.is_available():
                self.expression_generator = local_gen
                print("ğŸ  ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆè¡¨æƒ…")
            else:
                print("âš ï¸ æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œå›é€€åˆ° API æ¨¡å¼")
                self.expression_generator = ExpressionGenerator(config.llm)
        else:
            self.expression_generator = ExpressionGenerator(config.llm)
            print("ğŸŒ ä½¿ç”¨è¿œç¨‹ API ç”Ÿæˆè¡¨æƒ…")

        self.chat_generator = ChatGenerator(config.llm)  # èŠå¤©ç”Ÿæˆå™¨
        self.clients: Set[web.WebSocketResponse] = set()
        self.current_model: Optional[str] = None
        self.app = web.Application()
        self._setup_routes()
    
    def _setup_routes(self):
        """è®¾ç½®è·¯ç”±"""
        # æ ¹è·¯ç”± - æä¾› index.html
        self.app.router.add_get('/', self.serve_index)
        self.app.router.add_get('/ws', self.websocket_handler)
        self.app.router.add_get('/api/models', self.get_models)
        self.app.router.add_get('/api/config', self.get_config)
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹ç›®å½•æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡
        for model_dir in self.config.server.model_dirs:
            dir_path = Path(model_dir).resolve()
            dir_name = dir_path.name  # è·å–ç›®å½•åï¼Œå¦‚ "l2d", "hiyori"
            if dir_path.exists():
                self.app.router.add_static(f'/{dir_name}', str(dir_path))
                print(f"ğŸ“‚ é™æ€æ–‡ä»¶è·¯ç”±: /{dir_name} -> {dir_path}")
        
        # JS æ–‡ä»¶
        self.app.router.add_static('/js', './js')
        
        # èƒŒæ™¯å›¾ç‰‡ç›®å½•
        background_path = Path('./background').resolve()
        if background_path.exists():
            self.app.router.add_static('/background', str(background_path))
            print(f"ğŸ–¼ï¸ èƒŒæ™¯å›¾ç‰‡è·¯ç”±: /background -> {background_path}")
        
        # é…ç½® CORS
        cors = aiohttp_cors.setup(self.app, defaults={
            "*": aiohttp_cors.ResourceOptions(
                allow_credentials=True,
                expose_headers="*",
                allow_headers="*"
            )
        })
        
        for route in list(self.app.router.routes()):
            cors.add(route)
    
    async def serve_index(self, request):
        """æä¾› index.html é¦–é¡µ"""
        return web.FileResponse('./index.html')
    
    async def get_models(self, request):
        """HTTP API: è·å–æ¨¡å‹åˆ—è¡¨"""
        models = [asdict(m) for m in self.scanner.models.values()]
        return web.json_response({"models": models})
    
    async def get_config(self, request):
        """HTTP API: è·å–å‰ç«¯é…ç½®"""
        return web.json_response(self.config.get_frontend_config())
    
    async def websocket_handler(self, request):
        """WebSocket è¿æ¥å¤„ç†"""
        ws = web.WebSocketResponse()
        await ws.prepare(request)
        
        self.clients.add(ws)
        client_ip = request.remote
        print(f"ğŸ”— WebSocket å®¢æˆ·ç«¯è¿æ¥: {client_ip} (å½“å‰ {len(self.clients)} ä¸ª)")
        
        # å‘é€æ¨¡å‹åˆ—è¡¨
        await self._send_model_list(ws)
        
        try:
            async for msg in ws:
                if msg.type == aiohttp.WSMsgType.TEXT:
                    await self._handle_message(ws, msg.data)
                elif msg.type == aiohttp.WSMsgType.ERROR:
                    print(f"âŒ WebSocket é”™è¯¯: {ws.exception()}")
        finally:
            self.clients.discard(ws)
            print(f"ğŸ”Œ WebSocket å®¢æˆ·ç«¯æ–­å¼€: {client_ip} (å‰©ä½™ {len(self.clients)} ä¸ª)")
        
        return ws
    
    async def _send_model_list(self, ws: web.WebSocketResponse):
        """å‘é€æ¨¡å‹åˆ—è¡¨ç»™å®¢æˆ·ç«¯"""
        models = [asdict(m) for m in self.scanner.models.values()]
        await ws.send_json({
            "type": "model_list",
            "models": models,
            "current": self.current_model
        })
    
    async def _handle_message(self, ws: web.WebSocketResponse, data: str):
        """å¤„ç†å®¢æˆ·ç«¯æ¶ˆæ¯"""
        try:
            msg = json.loads(data)
            msg_type = msg.get("type")
            
            if msg_type == "load_model":
                # åŠ è½½æŒ‡å®šæ¨¡å‹
                model_name = msg.get("model")
                if model_name in self.scanner.models:
                    self.current_model = model_name
                    model = self.scanner.models[model_name]
                    await self._broadcast({
                        "type": "load_model",
                        "model": asdict(model)
                    })
                    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
                else:
                    await ws.send_json({
                        "type": "error",
                        "message": f"æ¨¡å‹ä¸å­˜åœ¨: {model_name}"
                    })
            
            elif msg_type == "update_parameters":
                # æ›´æ–°å¯ç”¨å‚æ•°ï¼ˆå‰ç«¯åŠ è½½æ¨¡å‹åå‘é€ï¼‰
                parameters = msg.get("parameters", {})
                self.expression_generator.update_parameters(parameters)
                await ws.send_json({
                    "type": "parameters_updated",
                    "count": len(parameters)
                })
            
            elif msg_type == "chat":
                # èŠå¤©æ¶ˆæ¯ï¼Œè§¦å‘ LLM ç”Ÿæˆè¡¨æƒ…
                text = msg.get("message", "")
                context = msg.get("context", "")
                auto_reset = msg.get("autoReset", True)
                
                try:
                    result = await self.expression_generator.generate(text, context)
                    
                    # å¹¿æ’­è¡¨æƒ…æŒ‡ä»¤ç»™æ‰€æœ‰å®¢æˆ·ç«¯
                    await self._broadcast({
                        "type": "expression",
                        "expression": result.get("expression", ""),
                        "parameters": result.get("parameters", {}),
                        "duration": result.get("duration", 800),
                        "autoReset": auto_reset
                    })
                    
                    print(f"ğŸ­ ç”Ÿæˆè¡¨æƒ…: {result.get('expression')}")
                    
                except Exception as e:
                    await ws.send_json({
                        "type": "error",
                        "message": str(e)
                    })
            
            elif msg_type == "chat_with_reply":
                # å®Œæ•´èŠå¤©ï¼šå¹¶å‘ç”Ÿæˆå¯¹è¯å›å¤å’Œè¡¨æƒ…
                text = msg.get("message", "")
                context = msg.get("context", "")
                history = msg.get("history", [])
                auto_reset = msg.get("autoReset", True)

                try:
                    total_start_time = time.time()
                    print(f"\n{'='*50}")
                    print(f"ğŸ“¨ æ”¶åˆ°èŠå¤©è¯·æ±‚: {text[:50]}...")

                    # å¹¶å‘è°ƒç”¨èŠå¤©å’Œè¡¨æƒ…ç”Ÿæˆ
                    chat_task = self.chat_generator.generate(text, history)
                    expression_task = self.expression_generator.generate(text, context)

                    # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
                    results = await asyncio.gather(
                        chat_task,
                        expression_task,
                        return_exceptions=True
                    )

                    chat_reply = results[0] if not isinstance(results[0], Exception) else f"èŠå¤©ç”Ÿæˆå¤±è´¥: {results[0]}"
                    expression_result = results[1] if not isinstance(results[1], Exception) else {}

                    total_elapsed = (time.time() - total_start_time) * 1000

                    # æ„å»ºå“åº”
                    response = {
                        "type": "chat_response",
                        "reply": chat_reply,
                        "expression": expression_result.get("expression", "") if isinstance(expression_result, dict) else "",
                        "parameters": expression_result.get("parameters", {}) if isinstance(expression_result, dict) else {},
                        "duration": expression_result.get("duration", 800) if isinstance(expression_result, dict) else 800,
                        "autoReset": auto_reset
                    }

                    # å‘é€ç»™è¯·æ±‚çš„å®¢æˆ·ç«¯
                    await ws.send_json(response)

                    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                    print(f"{'='*50}")
                    print(f"âœ… è¯·æ±‚å¤„ç†å®Œæˆ | æ€»è€—æ—¶: {total_elapsed:.0f}ms")
                    if isinstance(expression_result, dict):
                        print(f"   ğŸ­ è¡¨æƒ…: {expression_result.get('expression', 'æœªçŸ¥')}")
                    print(f"{'='*50}\n")

                except Exception as e:
                    print(f"âŒ èŠå¤©å¤„ç†é”™è¯¯: {e}")
                    await ws.send_json({
                        "type": "chat_error",
                        "error": str(e)
                    })
            
            elif msg_type == "expression":
                # ç›´æ¥è®¾ç½®è¡¨æƒ…å‚æ•°
                await self._broadcast({
                    "type": "expression",
                    "parameters": msg.get("parameters", {}),
                    "duration": msg.get("duration", 800),
                    "autoReset": msg.get("autoReset", False)
                })
            
            elif msg_type == "reset":
                # é‡ç½®è¡¨æƒ…
                await self._broadcast({
                    "type": "reset",
                    "duration": msg.get("duration", 800)
                })
            
            elif msg_type == "ping":
                await ws.send_json({"type": "pong"})
                
        except json.JSONDecodeError:
            await ws.send_json({
                "type": "error", 
                "message": "æ— æ•ˆçš„ JSON æ ¼å¼"
            })
        except Exception as e:
            print(f"âŒ å¤„ç†æ¶ˆæ¯é”™è¯¯: {e}")
            await ws.send_json({
                "type": "error",
                "message": str(e)
            })
    
    async def _broadcast(self, message: dict):
        """å¹¿æ’­æ¶ˆæ¯ç»™æ‰€æœ‰å®¢æˆ·ç«¯"""
        if not self.clients:
            return
        
        dead_clients = set()
        for ws in self.clients:
            try:
                await ws.send_json(message)
            except Exception:
                dead_clients.add(ws)
        
        self.clients -= dead_clients
    
    async def _on_model_change(self):
        """æ¨¡å‹æ–‡ä»¶å˜åŒ–å›è°ƒ"""
        await self._broadcast({
            "type": "model_list",
            "models": [asdict(m) for m in self.scanner.models.values()],
            "current": self.current_model
        })
    
    def start_watcher(self):
        """å¯åŠ¨æ–‡ä»¶ç›‘å¬"""
        event_handler = ModelWatcher(self.scanner, self._on_model_change)
        observer = Observer()
        
        for base_dir in self.scanner.base_dirs:
            if base_dir.exists():
                observer.schedule(event_handler, str(base_dir), recursive=True)
                print(f"ğŸ‘ï¸ ç›‘å¬ç›®å½•: {base_dir}")
        
        observer.start()
        return observer
    
    async def run(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        # åˆå§‹æ‰«æ
        self.scanner.scan_all()
        
        # å¯åŠ¨æ–‡ä»¶ç›‘å¬
        observer = self.start_watcher()
        
        # å¯åŠ¨ Web æœåŠ¡å™¨
        runner = web.AppRunner(self.app)
        await runner.setup()
        
        site = web.TCPSite(
            runner, 
            self.config.server.host, 
            self.config.server.port
        )
        
        await site.start()

        # ç¡®å®š LLM æ¨¡å¼æ˜¾ç¤ºä¿¡æ¯
        if self.config.llm.mode == "local":
            llm_info = "æœ¬åœ°æ¨¡å‹ (Qwen2.5 + LoRA)"
        else:
            llm_info = f"API ({self.config.llm.model})"

        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           SoulLink_Live2D Server å·²å¯åŠ¨                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸŒ Web ç•Œé¢:  http://localhost:{self.config.server.port:<24}â•‘
â•‘  ğŸ”Œ WebSocket: ws://localhost:{self.config.server.port}/ws{' ' * 21}â•‘
â•‘  ğŸ“ æ¨¡å‹ç›®å½•:  {str(self.config.server.model_dirs[0]):<31}â•‘
â•‘  ğŸ­ å·²å‘ç°æ¨¡å‹: {len(self.scanner.models):<30}â•‘
â•‘  ğŸ¤– è¡¨æƒ…ç”Ÿæˆ:  {llm_info:<31}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ æç¤º:
   - å°† Live2D æ¨¡å‹æ”¾å…¥æ¨¡å‹ç›®å½•ï¼ŒæœåŠ¡å™¨ä¼šè‡ªåŠ¨å‘ç°
   - åœ¨æµè§ˆå™¨æ§åˆ¶å°ä½¿ç”¨ SoulLink.chat("ä½ å¥½") æµ‹è¯•
   - æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨
   - åˆ‡æ¢æ¨¡å¼: ä¿®æ”¹ config.yaml ä¸­ llm.mode ä¸º "local" æˆ– "api"
        """)
        
        try:
            while True:
                await asyncio.sleep(3600)
        except asyncio.CancelledError:
            pass
        finally:
            observer.stop()
            observer.join()
            await runner.cleanup()

# ============================================
# ä¸»å…¥å£
# ============================================

def main():
    print("ğŸš€ SoulLink Server å¯åŠ¨ä¸­...")
    
    config = ConfigManager("config.yaml")
    server = SoulLinkServer(config)
    
    try:
        asyncio.run(server.run())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    main()
